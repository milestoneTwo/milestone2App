{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Text Complexity Classifier - Predict\n",
    "\n",
    "## Prediction notes\n",
    "\n",
    "This version of the notebook uses the test data to make predictions from the model generated in the dev notebook. It requires six pickles to be in the same directory and accepts one argument sys[0] which should be the test data.\n",
    "\n",
    "\n",
    "## Data import and cleaning\n",
    "\n",
    "Primary goals here are getting data from the training data set and creating metrics that will convey the complexity of the text to our classifiers. There are 3 additional data sources as part of the Kaggle set that we used. The average of acquisition data set contains information gathered on around 50,000 words and contains each words lemmatized root and information about when that word the average age a person learns that word and the frequency of its use. The concreteness ratings contains a smaller number of words, but gives an impression of how much a word is associated with a particular idea. Finally the dale_chall data set contains a list of words that are considered 'basic english'.\n",
    "\n",
    "\n",
    "#### Dependencies\n",
    "\n",
    "Below are the datacleaning dependencies only. A longer list of imports for modeling is at the start of that section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import statistics as stats\n",
    "import pickle as pkl\n",
    "import sys\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.word2vec import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(sys.argv[1])\n",
    "df = pd.read_csv('WikiLarge_Test.csv')\n",
    "def predict(df):\n",
    "    def tokenize_and_remove_stops(text):\n",
    "        text_list = word_tokenize(text)\n",
    "        stop_word_set = set(stopwords.words('english'))\n",
    "        clean_list = [word.lower() for word in text_list if word.lower() not in stop_word_set]\n",
    "        clean_list = [word for word in clean_list if re.match('^[a-z]+$', word)]\n",
    "\n",
    "        return clean_list\n",
    "    \n",
    "    def syllable_count(clean_text_list):\n",
    "        count = 0\n",
    "        for word in clean_text_list:\n",
    "            word = word.lower()\n",
    "            vowels = \"aeiouy\"\n",
    "            if word[0] in vowels:\n",
    "                count += 1\n",
    "            for index in range(1, len(word)):\n",
    "                if word[index] in vowels and word[index - 1] not in vowels:\n",
    "                    count += 1\n",
    "            if word.endswith(\"e\"):\n",
    "                count -= 1\n",
    "            if count == 0:\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    \n",
    "    def flesch_kincaid_ease(row):\n",
    "        return round(206.835 - 1.015*(row['word_count']) - 84.6*(row['avg_syll_per_word']),2)\n",
    "    \n",
    "    def get_mean_conc(text):\n",
    "        sum_conc = 0\n",
    "        word_count = max(len(text), 1)\n",
    "        for word in text:\n",
    "            try:\n",
    "                sum_conc += conc_dict[word]['Conc.M']\n",
    "            except:\n",
    "                sum_conc += mean_conc\n",
    "\n",
    "        return round(sum_conc / word_count, 4)\n",
    "\n",
    "    def get_mean_aoa(text):\n",
    "        sum_aoa = 0\n",
    "        word_count = max(len(text), 1)\n",
    "        for word in text:\n",
    "            try:\n",
    "                sum_aoa += age_dict[word]['aoa']\n",
    "            except:\n",
    "                sum_aoa += mean_aoa\n",
    "\n",
    "        return round(sum_aoa / word_count, 4)\n",
    "\n",
    "    def get_mean_perc(text):\n",
    "        sum_perc = 0\n",
    "        word_count = max(len(text), 1)\n",
    "        for word in text:\n",
    "            try:\n",
    "                sum_perc += age_dict[word]['perc_known']\n",
    "            except:\n",
    "                try:\n",
    "                    sum_perc += conc_dict[word]['Percent_known']\n",
    "                except:\n",
    "                    sum_perc += mean_perc_known\n",
    "\n",
    "        return round(sum_perc / word_count, 4)\n",
    "\n",
    "    def get_mean_freq(text):\n",
    "        list_freq = []\n",
    "        for word in text:\n",
    "            try:\n",
    "                list_freq.append(age_dict[word]['freq'])\n",
    "            except:\n",
    "                list_freq.append(med_freq)\n",
    "\n",
    "        if not list_freq:\n",
    "            list_freq.append(0)\n",
    "\n",
    "        return stats.median(list_freq)\n",
    "\n",
    "    def count_non_basic(text):\n",
    "        count = 0\n",
    "        for word in text:\n",
    "            try:\n",
    "                count += age_dict[word]['non_basic']\n",
    "            except:\n",
    "                try:\n",
    "                    count += conc_dict[word]['non_basic']\n",
    "                except:\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    def lem_combine(text_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        lem_word = []\n",
    "        for word in text_list:\n",
    "            lem_word.append(lem.lemmatize(word))\n",
    "\n",
    "        return (' '.join(word for word in lem_word))\n",
    "    \n",
    "    def document_vector(text):\n",
    "        doc = [word for word in text.split() if word in w2v.wv.vocab]\n",
    "        if len(doc) == 0:\n",
    "            doc.append('he')\n",
    "        return np.mean(w2v[doc])\n",
    "    \n",
    "    df['clean_text'] = df['original_text'].apply(tokenize_and_remove_stops)\n",
    "\n",
    "    df['syllables'] = df['clean_text'].apply(syllable_count)\n",
    "    df['word_count'] = df['clean_text'].apply(lambda x: len(x))\n",
    "    df['avg_syll_per_word'] = df['syllables'] / df['word_count']\n",
    "    df['avg_syll_per_word'] = df['avg_syll_per_word'].fillna(0)\n",
    "\n",
    "    \n",
    "    df['fc_ease'] = df.apply(flesch_kincaid_ease, axis = 1)\n",
    "\n",
    "    fc_mean = df['fc_ease'].dropna().mean()\n",
    "    df['fc_ease'] = df['fc_ease'].fillna(fc_mean)\n",
    "    \n",
    "    with open('concrete.pkl', 'rb') as handle:\n",
    "        conc_dict = pkl.load(handle) \n",
    "    \n",
    "     \n",
    "    \n",
    "    with open('aoa.pkl', 'rb') as handle:\n",
    "        age_dict = pkl.load(handle)\n",
    "        \n",
    "    with open('misc.pkl', 'rb') as handle:\n",
    "        misc_dict = pkl.load(handle)\n",
    "    \n",
    "    mean_conc = misc_dict['mean_conc']\n",
    "    mean_aoa = misc_dict['mean_aoa']\n",
    "    mean_perc_known = misc_dict['mean_perc_known']\n",
    "    med_freq = misc_dict['med_freq']\n",
    "        \n",
    "    df['mean_conc'] = df['clean_text'].apply(get_mean_conc)\n",
    "    df['mean_aoa'] = df['clean_text'].apply(get_mean_aoa)\n",
    "    df['mean_perc_known'] = df['clean_text'].apply(get_mean_perc)\n",
    "    df['mean_freq'] = df['clean_text'].apply(get_mean_freq)\n",
    "    df['non_basic_words'] = df['clean_text'].apply(count_non_basic)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    df['lem_text'] = df['clean_text'].apply(lem_combine)\n",
    "    \n",
    "    with open('vectorizer.pkl', 'rb') as handle:\n",
    "        vectorizer = pkl.load(handle)\n",
    "        \n",
    "    with open('log_reg.pkl', 'rb') as handle:\n",
    "        log_vec = pkl.load(handle)\n",
    "        \n",
    "    X_vec = vectorizer.transform(df['lem_text'])\n",
    "    \n",
    "    df['logreg_prob'] = [num[0] for num in log_vec.predict_proba(X_vec)]\n",
    "    \n",
    "    with open('w2v.pkl', 'rb') as handle:\n",
    "        w2v = pkl.load(handle)\n",
    "        \n",
    "    df['w2v'] = df.lem_text.apply(document_vector)\n",
    "    \n",
    "    X_test = df[['word_count', 'avg_syll_per_word', 'fc_ease', 'mean_conc', 'mean_aoa', 'mean_perc_known', 'mean_freq', 'non_basic_words', 'logreg_prob', 'w2v']]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    with open('knn.pkl', 'rb') as handle:\n",
    "        knn_clf = pkl.load(handle)\n",
    "    \n",
    "    df['label'] = knn_clf.predict(X_test_scaled)\n",
    "    df['id'] = df.index\n",
    "    \n",
    "    df_return = df[['id', 'label']]\n",
    "    \n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e131353\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:105: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "df_test = predict(df)\n",
    "df_test.to_csv('predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
