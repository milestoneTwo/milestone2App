{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Milestone 2 Project Brought to you by Dave Franks and Ermias Bizuwork About MADS Program 2021 Go Blue!! About the Project The project was broken up into two parts based on the type of learning being employed. The first part Part A is the supervised learning portion where the goal was to build a binary classifier to beat some derived estimators at predicting the level of reading difficulty of text. The second part Part B is the unsupervised learning portion of the project. Where using the same data the goal was to use unsupervised learning techniques to glean some information from the dataset. In summary there were two main purposes to the project. Part A : To build a model that out performs in accuracy metric the Logistic Regression Classifier and Naive Bayes Classifier scoring 68% and 65% accuracy respectively. Part B : Create an unsupervised learning model that can characterize and explain the dataset in some way. The third and final stretch goal was to combine the two parts to have a semi-supervised algorithm to better increase the accuracy of the binary classifier made in Part A. The idea behind was to build a feature using an unsupervised topic model like LDA to create a feature that would be aggregated with other features in Part A. Due to some technical limitations it may not be feasible to accomplish this portion of the task but a description of what could be done is described in the Stretch Goals Using the Project <<<<<<< Updated upstream A good starting point which you have already achieved if you are seeing this message is going to the github repo and following the instructions in the README.md. This will help you get the project setup if you haven't already and get your development environment ready to go. ======= A good starting is going to the github repo and following the instructions in the README.md. This will help you get the project setup if you haven't already and get your development environment ready to go. Stashed changes Installation Steps To Setup Local Environment Clone Directory git clone https://github.com/milestoneTwo/milestone2App.git Change directory inside the directory cd [project directory] Make virtual environment python3 -m venv m2venv Jump inside the virtual environment source m2venv/bin/activate Install requirements pip install -r requirements.txt Run Steps To run the development web server run the following commands cd Milestone2Docs && mkdocs serve Or python app.py --run app Build the app with source data { DO NOT DO THIS } python app.py --run build_app Project Design Notes","title":"Home"},{"location":"#milestone-2-project","text":"Brought to you by Dave Franks and Ermias Bizuwork About MADS Program 2021 Go Blue!!","title":"Milestone 2 Project"},{"location":"#about-the-project","text":"The project was broken up into two parts based on the type of learning being employed. The first part Part A is the supervised learning portion where the goal was to build a binary classifier to beat some derived estimators at predicting the level of reading difficulty of text. The second part Part B is the unsupervised learning portion of the project. Where using the same data the goal was to use unsupervised learning techniques to glean some information from the dataset. In summary there were two main purposes to the project. Part A : To build a model that out performs in accuracy metric the Logistic Regression Classifier and Naive Bayes Classifier scoring 68% and 65% accuracy respectively. Part B : Create an unsupervised learning model that can characterize and explain the dataset in some way. The third and final stretch goal was to combine the two parts to have a semi-supervised algorithm to better increase the accuracy of the binary classifier made in Part A. The idea behind was to build a feature using an unsupervised topic model like LDA to create a feature that would be aggregated with other features in Part A. Due to some technical limitations it may not be feasible to accomplish this portion of the task but a description of what could be done is described in the Stretch Goals","title":"About the Project"},{"location":"#using-the-project","text":"<<<<<<< Updated upstream A good starting point which you have already achieved if you are seeing this message is going to the github repo and following the instructions in the README.md. This will help you get the project setup if you haven't already and get your development environment ready to go. ======= A good starting is going to the github repo and following the instructions in the README.md. This will help you get the project setup if you haven't already and get your development environment ready to go. Stashed changes","title":"Using the Project"},{"location":"#installation-steps","text":"","title":"Installation Steps"},{"location":"#to-setup-local-environment","text":"Clone Directory git clone https://github.com/milestoneTwo/milestone2App.git Change directory inside the directory cd [project directory] Make virtual environment python3 -m venv m2venv Jump inside the virtual environment source m2venv/bin/activate Install requirements pip install -r requirements.txt","title":"To Setup Local Environment"},{"location":"#run-steps","text":"To run the development web server run the following commands cd Milestone2Docs && mkdocs serve Or python app.py --run app Build the app with source data { DO NOT DO THIS } python app.py --run build_app","title":"Run Steps"},{"location":"#project-design-notes","text":"","title":"Project Design Notes"},{"location":"about/","text":"Team Members Dave Franks Ermias Bizuwork","title":"About"},{"location":"about/#team-members","text":"","title":"Team Members"},{"location":"about/#dave-franks","text":"","title":"Dave Franks"},{"location":"about/#ermias-bizuwork","text":"","title":"Ermias Bizuwork"},{"location":"part1/","text":"Text Complexity Classifier and Topic Modeling General Project Approach and Division of Work For this project, the first published Kaggle dataset for text complexity was analyzed. Both a supervised learning model which would predict text that needed simplification and an unsupervised portion that looked at topic modeling were created and evaluated. As the project was already split into two major sections, these sections were used to determine the division of work. David Franks worked on the supervised text classifier, while Ermias Bizuwork handled the unsupervised topic modeling, and set up a github repository to help with organization. Each also served as a consultant for any issues that arose in the other\u2019s section. Supervised Text Classifier Three Step Approach For organization the supervised text classifier and predictions were generated across three separate notebooks. In a professional setting, and with more time, the goal would be to turn these three notebooks into a pipeline. The first notebook is the POC (proof of concept) notebook. The goal of the POC notebook was to solidify the feature generation and isolate a single high performing model on the training data using train/test splits. The full training data is in excess of 400,000 rows and can be quite costly for training models and grid searching their parameters. For this reason, through the initial feature generation and model evaluation 20,000 rows were used at random to train, then 20,000 different rows were used at random to test. In text classification tasks, the final performance of the model can be heavily dependent on the size of the training set. Initial train/test splits showed that to be the case here. Once a few promising models were identified on the smaller data set, final model tuning and evaluation was performed on the full training data with a 75/25 train test split. With feature generation complete and a model and its parameters selected, the Dev (development) notebook was instantiated. The goal of the Dev notebook was to train a final model on 100 percent of the training data. The model and all relevant dictionaries were then output to pickle files. The final notebook, Predict, was written to easily be reformatted to a callable script that would accept two arguments, a text file with rows of text needing classification, and a destination for the predictions. The Predict notebook innately pulls the pickles generated by the Dev notebook, these however could also be passed as arguments if a full pipeline were made. Feature Generation Following is a full list of features used. The methodologies by which they were generated can be found in the appendix of this paper. Word Count & Average Syllable per Word (word_count, avg_syll_per_word). Syllable counts were generated through a function taking from a stack overflow thread (top answer at this link) . Flesch-Kincaid Reading Ease Score (fc_ease) Mean Word Concreteness Rating (mean_conc) Mean Word Age of Acquisition (mean_aoa) Mean Word Percent Known (mean_perc) Median Word Frequency (mean_freq) Count of Non-Basic words (non_basic_words) Word2Vec Mean Document Vector (w2v) TfIdf Logistic Regression Probability (log_reg_prob) Evaluation Feature representation grew in number and complexity through iterations of model testing. The first version had only the word count, syllables and Flesch-Kincaid score. This was seen to have accuracy in lower 60th percentile. Within the models tested, a Random and Gradient Boosting forest were run on every iteration. From these, the feature_importances_ method was used to evaluate the features impact on the model. As more features were developed, the importance of features was reevaluated, low importance features were often dropped to assess if they were negatively impacting the model, and features thought to be heavily co-linear (e.g. word count and count of non-basic words) were added and removed to assess if they lead contributed to information gain. Once all the non-vector-based features were added, the best model (random forest classifier) had approximately 69 percent accuracy. Looking to go past that, vector-based representations of the text were assessed. A TfIdf vectorizer was fit to the training data, and its sparse matrix was used to train a logistic regression classifier. This representation had 68.5 percent accuracy on its own. Given that vector representation and the features representing the attributes of the text came out to equally strong classifiers, through very different methods, it was suspected that if they could be combined, the result would be better than either individually. The TfIdf logistic regression model was employed to predict the probability of class assignment for each row. This probability was then used as an additional feature added to the text attributes. Similarly, a word2vec model was trained and the average document vector was added. With the two additional features, all models saw increased accuracy. There was a small shift in which a KNN classifier began outperforming the other methods at around 72 percent accuracy when trained on a 75/25 split of the data. Hyperparameter tuning was performed and a final model using 100 nearest neighbors weighted by distance showed the best performance across 10 cross fold validation. This process did add a small gain to accuracy (less the 0.5 percent) and took several hours to complete. Additionally, even with 10 cross fold validations, the recommended parameters from the grid search were not always the same. This indicates that a standard trope of machine learning is true in this case: feature selection heavily outweighs hyperparameter tuning in terms of model accuracy. Even most classifier types performed in a cluster around the 70-72 percent accuracy range with a gradient boosting forest, random forest, k-nearest neighbors, and a second logistic regression classifier all in that range. This also shows that feature importance outweighs model selection in terms of final accuracy, provided the model is appropriate to the data. A gaussian mixture model for instance, only had 49.5 percent accuracy, indicating it failed to capture any of the underlying structure in the data. Final Feature Importances Below are violin plots of each feature, split by outcome on 20,000 random rows. The log_reg_prob feature, which represents the probability of class assignment from the logistic regression model run on the TfIdf vectorization, is by far the most influential feature. Feature importance metrics placed this single feature at 0.553 with the next most important feature being the word2vec average document vector at 0.062. Text based features all sat between 0.037 (average syllables per word) and 0.0573 (mean age of acquisition). Failure Analysis With the inexact qualities of language, there were many cases and many reasons why predictions failed. Below are the three most common cases observed. Failure of logistic regression probability The log_reg_prob feature was the feature identified with the highest importance. However, it was a probability prediction from a classifier with only 68.5 percent accuracy. The TfIdf methodology can be hard to analyze for failure states, but it would identify texts that had words in common with texts needing to be simplified as also needing simplification. -Simple sentences with many words and non-basic words General word count and a count of words not seen in the dale_chall file of basic words both entered the model. This may have inadvertently weighted the model to over identify long sentences as needing simplification. This can be seen by flipping the class predictions (simple = 1) and looking at a graph of the recall score by word count. Flesch-Kincaid Ease not heavily correlated Initial thoughts going into this project were that this feature would be highly predictive of text complexity. Early versions of the classifier that relied on only text features had underwhelming accuracy. The final version of the model does not rely heavily on the fc_ease feature and as can be seen in the violin plots above; the distribution across the classes is similar. A lower fc_ease value was still associated with the \u2018needs simplification\u2019 class, but this is another spot where it was common for the model to fail. Cases were seen where both the ease was low, but the sentence was simple, and where the ease was high. but the sentence was considered complex. Part A Discussion Lessons Learned It became obvious toward the beginning of evaluation that features relying solely on common text attributes like counts and syllables, were insufficient in capturing the complexity of the text. Flesch-Kincaid Ease was particularly disappointing as a feature. Adding the additional resources like mean concreteness and age of acquisition helped, but even with those external databases, a model using these features was on par for performance with a tfidf vectorizer and logistic regression all with default parameters. With more time, vectorization methods could be explored further. Pretrained vectorizes could be investigated to see if they perform better. The issue of word count being too heavily weighted could also be explored, perhaps by changing the count of non-basic words for a percent of non-basic words to avoid strong co-linearity. Ethical Considerations Since the articulated goal of the classifier is to identify texts that need to be simplified by Wikipedia staff, there are a few ethical concerns. Containing infrequently used words was a factor in the text being flagged as needing review. Given that there are words associated with cultural practices, it\u2019s not hard to imagine that text revolving around practices seen in minority cultures would be flagged more often. If this was shown to be the case and review of this classifier in a real-world setting found a disproportionate flagging of articles about minority cultures as needing to be simplified, the classifier would need to be used with a set of criteria seeks to avoid replacing traditional words and proper names with more generic language. Assuring that definitions were given for these words in the text itself may be a way to preserve the words themselves, while promoting understanding of the text. If these words were frequently changed, accusations of historical and cultural sanitation in favor of majority understanding could rightly be levied. Appendix A: Explanation of features and how they were created: - Word & Average Syllable per Word (word_count, avg_syll_per_word) While syllable counts were available in the age of acquisition and concreteness rating documents, those documents did not contain a complete list of all words used. Word counts were generated by taking the length of the clean_text column. Syllable counts were generated through a manual function taking from a stack overflow thread (top answer at this link). Syllable count was then used with the word count to calculate average syllables per word. Word and syllable counts were not performed on the lemmatized text as they were intended to reflect more of the original state of the document than the more heavily processed lem_text. Version of syllable and word counts that still contained stop words were tried in models, but those models appeared to be slightly less accurate, potential as a result of the Flesch-Kincaid score discussed below. Flesch-Kincaid Reading Ease Score (fc_ease) Once the word and syllable counts were established, an average syllables per word was calculated for use in a Flesch-Kincaid Reading Ease Score. As many stop words are a single syllable and were removed prior to calculating the average syllables per word in each text, the final flesch-kincaid scores were significantly lower than version that scored with stop words. This version was seen to be slightly more accurate when predicting text complexity, possibly as the remaining words complexity being less diluted by the stop words. Mean Concreteness Rating (mean_conc) Using the dictionary representation of the concreteness rating data source, the function get_mean_conc(text) took the sum of concreteness ratings for words in the text and divided by the number of words in the text. If a word in the text did not appear in the concreteness dictionary, it\u2019s value was imputed using the average concreteness rating within the dictionary. Mean Age of Acquisition (mean_aoa) Using the dictionary representation of the AoA datasource, this was calculated in a manner similar to the mean concreteness. Average age of acquisition for each word was summed, the divided by total words in the text. Words that did not appear in the AoA datasource used the mean age across all words in that datasource. In an attempt to limit the number of missing words, alternative spellings in the AoA datasource were made into their own unique entries within the dictionary. Mean Percent Known (mean_perc) This metric was available from both the AoA and Concreteness Rating datasources. While the AoA source had the large list of words, there were some unique words found in the concreteness ratings. As it had the larger list, the AoA datasource was checked first for percent known values, if the word did not exist there the Concreteness Ratings data source was checked. If a word did not appear in either datasource it was imputed with the average percent known from the AoA datasouce. Median Frequency (mean_freq) Do to the dynamic nature of this measure, revision of the code saw this change to a median metric rather than arithmetic mean. Words were associated with their values in the AoA datasource, missing values were imputed from the median of all values in that datasource. Count of Non-Basic words (non_basic_words) This metric is a sum of non-basic words used in the text. It\u2019s calculated by looking through both the AoA and Concreteness Ratings dictionaries which had non_basic metric entered into them, based on whether the word appeared in the dale_chall file. If a word appears in neither dictionary, it is assumed to be not basic. Word2Vec Mean Document Vector Using the lem_text column, a word2vec vectorizer was trained. From this, the average document vector was used as feature. TfIdf Logistic Regression Probability (log_reg_prob) Early in development, it was noticed that a simple TfIdf vectorizer passed through a logistic regression model with default parameters had relatively high accuracy on its own. Given that TfIdf method is distinct from the text features listed above, it was estimated that combining this method with the other features would be beneficial. However, since the TfIdf vectorizer yields a sparse matrix, it was not readily consumable by models alongside the metrics. A logistic regression model was trained on the test data and was used to predict the probability of the class assignment. This probability was passed as a feature along with those listed above, into the final model. Accuracy improved from approximately 68 \u2013 70 percent for either text features or TfIdf to approximately 74.5 percent when using a two-stage model.","title":"Part1"},{"location":"part1/#text-complexity-classifier-and-topic-modeling","text":"","title":"Text Complexity Classifier and Topic Modeling"},{"location":"part1/#general-project-approach-and-division-of-work","text":"For this project, the first published Kaggle dataset for text complexity was analyzed. Both a supervised learning model which would predict text that needed simplification and an unsupervised portion that looked at topic modeling were created and evaluated. As the project was already split into two major sections, these sections were used to determine the division of work. David Franks worked on the supervised text classifier, while Ermias Bizuwork handled the unsupervised topic modeling, and set up a github repository to help with organization. Each also served as a consultant for any issues that arose in the other\u2019s section.","title":"General Project Approach and Division of Work"},{"location":"part1/#supervised-text-classifier","text":"","title":"Supervised Text Classifier"},{"location":"part1/#three-step-approach","text":"For organization the supervised text classifier and predictions were generated across three separate notebooks. In a professional setting, and with more time, the goal would be to turn these three notebooks into a pipeline. The first notebook is the POC (proof of concept) notebook. The goal of the POC notebook was to solidify the feature generation and isolate a single high performing model on the training data using train/test splits. The full training data is in excess of 400,000 rows and can be quite costly for training models and grid searching their parameters. For this reason, through the initial feature generation and model evaluation 20,000 rows were used at random to train, then 20,000 different rows were used at random to test. In text classification tasks, the final performance of the model can be heavily dependent on the size of the training set. Initial train/test splits showed that to be the case here. Once a few promising models were identified on the smaller data set, final model tuning and evaluation was performed on the full training data with a 75/25 train test split. With feature generation complete and a model and its parameters selected, the Dev (development) notebook was instantiated. The goal of the Dev notebook was to train a final model on 100 percent of the training data. The model and all relevant dictionaries were then output to pickle files. The final notebook, Predict, was written to easily be reformatted to a callable script that would accept two arguments, a text file with rows of text needing classification, and a destination for the predictions. The Predict notebook innately pulls the pickles generated by the Dev notebook, these however could also be passed as arguments if a full pipeline were made.","title":"Three Step Approach"},{"location":"part1/#feature-generation","text":"Following is a full list of features used. The methodologies by which they were generated can be found in the appendix of this paper. Word Count & Average Syllable per Word (word_count, avg_syll_per_word). Syllable counts were generated through a function taking from a stack overflow thread (top answer at this link) . Flesch-Kincaid Reading Ease Score (fc_ease) Mean Word Concreteness Rating (mean_conc) Mean Word Age of Acquisition (mean_aoa) Mean Word Percent Known (mean_perc) Median Word Frequency (mean_freq) Count of Non-Basic words (non_basic_words) Word2Vec Mean Document Vector (w2v) TfIdf Logistic Regression Probability (log_reg_prob)","title":"Feature Generation"},{"location":"part1/#evaluation","text":"Feature representation grew in number and complexity through iterations of model testing. The first version had only the word count, syllables and Flesch-Kincaid score. This was seen to have accuracy in lower 60th percentile. Within the models tested, a Random and Gradient Boosting forest were run on every iteration. From these, the feature_importances_ method was used to evaluate the features impact on the model. As more features were developed, the importance of features was reevaluated, low importance features were often dropped to assess if they were negatively impacting the model, and features thought to be heavily co-linear (e.g. word count and count of non-basic words) were added and removed to assess if they lead contributed to information gain. Once all the non-vector-based features were added, the best model (random forest classifier) had approximately 69 percent accuracy. Looking to go past that, vector-based representations of the text were assessed. A TfIdf vectorizer was fit to the training data, and its sparse matrix was used to train a logistic regression classifier. This representation had 68.5 percent accuracy on its own. Given that vector representation and the features representing the attributes of the text came out to equally strong classifiers, through very different methods, it was suspected that if they could be combined, the result would be better than either individually. The TfIdf logistic regression model was employed to predict the probability of class assignment for each row. This probability was then used as an additional feature added to the text attributes. Similarly, a word2vec model was trained and the average document vector was added. With the two additional features, all models saw increased accuracy. There was a small shift in which a KNN classifier began outperforming the other methods at around 72 percent accuracy when trained on a 75/25 split of the data. Hyperparameter tuning was performed and a final model using 100 nearest neighbors weighted by distance showed the best performance across 10 cross fold validation. This process did add a small gain to accuracy (less the 0.5 percent) and took several hours to complete. Additionally, even with 10 cross fold validations, the recommended parameters from the grid search were not always the same. This indicates that a standard trope of machine learning is true in this case: feature selection heavily outweighs hyperparameter tuning in terms of model accuracy. Even most classifier types performed in a cluster around the 70-72 percent accuracy range with a gradient boosting forest, random forest, k-nearest neighbors, and a second logistic regression classifier all in that range. This also shows that feature importance outweighs model selection in terms of final accuracy, provided the model is appropriate to the data. A gaussian mixture model for instance, only had 49.5 percent accuracy, indicating it failed to capture any of the underlying structure in the data.","title":"Evaluation"},{"location":"part1/#final-feature-importances","text":"Below are violin plots of each feature, split by outcome on 20,000 random rows. The log_reg_prob feature, which represents the probability of class assignment from the logistic regression model run on the TfIdf vectorization, is by far the most influential feature. Feature importance metrics placed this single feature at 0.553 with the next most important feature being the word2vec average document vector at 0.062. Text based features all sat between 0.037 (average syllables per word) and 0.0573 (mean age of acquisition).","title":"Final Feature Importances"},{"location":"part1/#failure-analysis","text":"With the inexact qualities of language, there were many cases and many reasons why predictions failed. Below are the three most common cases observed. Failure of logistic regression probability The log_reg_prob feature was the feature identified with the highest importance. However, it was a probability prediction from a classifier with only 68.5 percent accuracy. The TfIdf methodology can be hard to analyze for failure states, but it would identify texts that had words in common with texts needing to be simplified as also needing simplification. -Simple sentences with many words and non-basic words General word count and a count of words not seen in the dale_chall file of basic words both entered the model. This may have inadvertently weighted the model to over identify long sentences as needing simplification. This can be seen by flipping the class predictions (simple = 1) and looking at a graph of the recall score by word count. Flesch-Kincaid Ease not heavily correlated Initial thoughts going into this project were that this feature would be highly predictive of text complexity. Early versions of the classifier that relied on only text features had underwhelming accuracy. The final version of the model does not rely heavily on the fc_ease feature and as can be seen in the violin plots above; the distribution across the classes is similar. A lower fc_ease value was still associated with the \u2018needs simplification\u2019 class, but this is another spot where it was common for the model to fail. Cases were seen where both the ease was low, but the sentence was simple, and where the ease was high. but the sentence was considered complex.","title":"Failure Analysis"},{"location":"part1/#part-a-discussion","text":"Lessons Learned It became obvious toward the beginning of evaluation that features relying solely on common text attributes like counts and syllables, were insufficient in capturing the complexity of the text. Flesch-Kincaid Ease was particularly disappointing as a feature. Adding the additional resources like mean concreteness and age of acquisition helped, but even with those external databases, a model using these features was on par for performance with a tfidf vectorizer and logistic regression all with default parameters. With more time, vectorization methods could be explored further. Pretrained vectorizes could be investigated to see if they perform better. The issue of word count being too heavily weighted could also be explored, perhaps by changing the count of non-basic words for a percent of non-basic words to avoid strong co-linearity. Ethical Considerations Since the articulated goal of the classifier is to identify texts that need to be simplified by Wikipedia staff, there are a few ethical concerns. Containing infrequently used words was a factor in the text being flagged as needing review. Given that there are words associated with cultural practices, it\u2019s not hard to imagine that text revolving around practices seen in minority cultures would be flagged more often. If this was shown to be the case and review of this classifier in a real-world setting found a disproportionate flagging of articles about minority cultures as needing to be simplified, the classifier would need to be used with a set of criteria seeks to avoid replacing traditional words and proper names with more generic language. Assuring that definitions were given for these words in the text itself may be a way to preserve the words themselves, while promoting understanding of the text. If these words were frequently changed, accusations of historical and cultural sanitation in favor of majority understanding could rightly be levied.","title":"Part A Discussion"},{"location":"part1/#appendix-a","text":"Explanation of features and how they were created: - Word & Average Syllable per Word (word_count, avg_syll_per_word) While syllable counts were available in the age of acquisition and concreteness rating documents, those documents did not contain a complete list of all words used. Word counts were generated by taking the length of the clean_text column. Syllable counts were generated through a manual function taking from a stack overflow thread (top answer at this link). Syllable count was then used with the word count to calculate average syllables per word. Word and syllable counts were not performed on the lemmatized text as they were intended to reflect more of the original state of the document than the more heavily processed lem_text. Version of syllable and word counts that still contained stop words were tried in models, but those models appeared to be slightly less accurate, potential as a result of the Flesch-Kincaid score discussed below. Flesch-Kincaid Reading Ease Score (fc_ease) Once the word and syllable counts were established, an average syllables per word was calculated for use in a Flesch-Kincaid Reading Ease Score. As many stop words are a single syllable and were removed prior to calculating the average syllables per word in each text, the final flesch-kincaid scores were significantly lower than version that scored with stop words. This version was seen to be slightly more accurate when predicting text complexity, possibly as the remaining words complexity being less diluted by the stop words. Mean Concreteness Rating (mean_conc) Using the dictionary representation of the concreteness rating data source, the function get_mean_conc(text) took the sum of concreteness ratings for words in the text and divided by the number of words in the text. If a word in the text did not appear in the concreteness dictionary, it\u2019s value was imputed using the average concreteness rating within the dictionary. Mean Age of Acquisition (mean_aoa) Using the dictionary representation of the AoA datasource, this was calculated in a manner similar to the mean concreteness. Average age of acquisition for each word was summed, the divided by total words in the text. Words that did not appear in the AoA datasource used the mean age across all words in that datasource. In an attempt to limit the number of missing words, alternative spellings in the AoA datasource were made into their own unique entries within the dictionary. Mean Percent Known (mean_perc) This metric was available from both the AoA and Concreteness Rating datasources. While the AoA source had the large list of words, there were some unique words found in the concreteness ratings. As it had the larger list, the AoA datasource was checked first for percent known values, if the word did not exist there the Concreteness Ratings data source was checked. If a word did not appear in either datasource it was imputed with the average percent known from the AoA datasouce. Median Frequency (mean_freq) Do to the dynamic nature of this measure, revision of the code saw this change to a median metric rather than arithmetic mean. Words were associated with their values in the AoA datasource, missing values were imputed from the median of all values in that datasource. Count of Non-Basic words (non_basic_words) This metric is a sum of non-basic words used in the text. It\u2019s calculated by looking through both the AoA and Concreteness Ratings dictionaries which had non_basic metric entered into them, based on whether the word appeared in the dale_chall file. If a word appears in neither dictionary, it is assumed to be not basic. Word2Vec Mean Document Vector Using the lem_text column, a word2vec vectorizer was trained. From this, the average document vector was used as feature. TfIdf Logistic Regression Probability (log_reg_prob) Early in development, it was noticed that a simple TfIdf vectorizer passed through a logistic regression model with default parameters had relatively high accuracy on its own. Given that TfIdf method is distinct from the text features listed above, it was estimated that combining this method with the other features would be beneficial. However, since the TfIdf vectorizer yields a sparse matrix, it was not readily consumable by models alongside the metrics. A logistic regression model was trained on the test data and was used to predict the probability of the class assignment. This probability was passed as a feature along with those listed above, into the final model. Accuracy improved from approximately 68 \u2013 70 percent for either text features or TfIdf to approximately 74.5 percent when using a two-stage model.","title":"Appendix A:"},{"location":"part2/","text":"PartB Unsupervised Learning Motivation In this part of the project our goal was to use Latent Dirichlet Allocation to \"cluster\" or create topcis which aggregate the documents in the dataset together for analysis. Motivation for the work in this section was to understand how to be use a topic model to extract a useful feature to then be able to fuel the classifier in Part A of this project. In doing so more information can be gleaned from understanding word pairings and probabilities distributions of documents to derived topics. More analysis is needed to use the topic as a feature for the binary classifier in Part A. This is discussed in the Stretch Goals on how this could have been accomplished. Data Source The sources for this data were provided by the instruction team for SIADS694 and SIADS695. Details on the dataset can be found in the kaggle competition . The dataset was downloaded using the kaggle dowloader python tool from the kaggle competition website. The dataset consisted of a few different files that were designed to be used as potential features for PartA. For PartB however we only needed to use the csv files with the sentences in them. Those files are named 'WikiLarge_Test.csv' and 'WikiLarge_Train.csv'. In the 'WikiLarge_Train.csv' there are two columns one called 'original_text' and one called 'label'. Original text is a string representing the document and label is its classification as an integer (0,1) as either 'does NOT need to be simplified' or 'does need to be simplified'. * 'WikiLarge_Train.csv' has 416,768 documents in the corpus (labeled) * 'WikiLarge_Test.csv' has 119,092 documents in the corpus (unlabeled) Unsupervised Learning Methods Our main goal here was to process the training documents to extract feature representations for PartA. Given time constraints this goal was not achieved fully. However it did yield some interesting findings going through the effort. It was a great challenge engineering wise working with a large dataset and performing some of the analysis needed to extract topics using LDA. Model tuning and other processing steps came at somewhat of a premium as well given how long it takes to make some of the steps. Source Code Workflow The methodology was similar to most Natural Language Processing tasks. There is a fairly discrete high level pipeline with potential modifications at each step that are dependent on the task at hand. Starting with the high level architecture. Given the amount of time spent coding and recoding or pickling objects a set of utilities were built in the form of a python package or library. It is title m2lib and is in the main project directory. It houses the picklers, preprocessors, featureizers, and model creating code. Pickling An important note here is that at each step instead of re-performing each sub step in the process great effort was taken to ensure that repeating the process was not necessary. Custom classes were built such that when inherited from they could check for presence of a pickle corresponding to the child class before initializing the object. This meant the whole class (object) would get serialized and deserialized automatically on init. Which isn't always desireable given that those classes may change when you are developing. So some work is needed to refine the architecture. A shortcut was taken to create a separate picklable object that could store the important output of each step. In the cases encountered here it was typically 5 types. Preprocessed tokens example # tokens ['the', 'car', 'was', 'fast'] #ngamrms (bigrams) ['the_car', 'car_was', 'was_fast'] # full token token + bigram ['the', 'car', 'was', 'fast', 'the_car', 'car_was', 'was_fast'] Featureized Tokens # bag of words (id, count) [(13, 1), (53, 3), (18, 4), (52, 3)] #TFID vectors [[[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]] ['addition', 'anna', 'austen', 'continued', 'continued_work'] # dictionary (gensim utils) Models (objects) HTML (charts) Native Python objects Basic Pipeline Steps Read data Preprocess (tokenize, lemmatize, stopwords, punctuation removal, etc) Create feature representations (Bag of words, vectors) Create Model Analyze Model Visualize Overview Step 1 we are reading the data into memory using pandas read from csv functionality. Its not a particular costly process and can be done quickly. Still a class was created to handle reading and accessing dataframes and original data then serialized to pickle. Step 2 is preprocessing it was done mostly with gensim preprocessing classes that were very useful and comprehensible. SpaCy was a bit more difficult to fit into the design without going to far into their architecture. However given that is has some very advanced techniques for processing data it will be considered for future use. Their pre built pipelines are supposed to be fast and intelligent. Tokenizing code. As you can see two popular steps for tokenizing were left out which is stripping short tokens and stemming text. This decision was made for cleanliness of output from LDA model having legible words. def tokenize_gensim_string(self, doc): CUSTOM_FILTERS = [ strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, # strip_short, # stem_text ] doc_ = preprocess_string(doc, CUSTOM_FILTERS) return doc_ Step 3 Creating feature representation was also done with gensim using the bag of words model and TFID model. Each of which produce their own object representations as output that can be used for LDA model creation. See gensim tfidmodel and doc2bow Step 4 Creating models again was another task covered by gensim. LDA Model It's one of the more widely used for LDA and has tons of support. A model was created with bag of words and tfid to compare the differences in the two. A third model was created using Gibbs Sampling Dirichlet Multinomial Mixture which is supposed to be better at topic modeling for short texts. It is also known as the Movie Group Process. Described in this medium post . Step 5 Analyzing the models was done with using two metrics perplexity and coherence measures using gensim's LDA model attributes and methods. Step 6 Visualizations were done primarily with PyLDAvis . It can hook right into the model and provide insight into the 2D cluster structure of the topic model. Parameter Tuning The next section discusses a little in more detail on evaluation and how it impacted parameter tuning. There were few parameters to tune that had meaningful effects on the outcomes. The most impactful parameter is by far the number of K topics selected for each topic model (LDA, GSDMM). For bag of words feature there are some parameters to filter out extreme values from the dataset that are either lower or higher in instance count. LDA model has a few parameters that can be tuned most were left default. self.lda_args = { 'chunksize': 2000, 'alpha' : 'auto', 'eta': 'auto', 'iterations' : 50, 'num_topics' : 10, # K 'passes' : 20, 'eval_every' : None, # 'workers' : 3, } To include or not include ngrams? That question was difficult to answer with higher order ngrams your doc term matrix ends up being very sparse. So I split the middle and went with only 2 ngram model. Once this was selected it remained stationary for the project as running the bag of words model was costly. Unsupervised Evaluation and Parameter Tuning Evaluation was done at a few levels in the basic pipeline described above for picking some of the parameters and hyperparameters. However the largest amount of effort went into evaluating perplexity and coherence. Higher coherence is better and lower perplexity is better. Given time constraints only a few runs were made at trying to select an ideal K number of topics. The algorithm performed was to iterate through 4 topic sizes [5,10,15,20] to then calculate perplexity and coherence and optimize the topic selection based on the outcomes. Discussion There were several learnings from this project. One of the greatest is that large datasets are very difficult and time consuming to running analysis on. The gensim implementation of bag of words calculation took 10+ hours to complete on the final run. So pickling was a high priority in order to prevent from having to re run the pipeline steps for testing. The solution has several opportunities for expansion one mentioned in the section for stretch goals . Second the project overall could be integrated partA and partB in order to build a toolset that can run a full semisupervised pipeline with feature enhancements using topics. Lastly expand the pipeline, class structure, and architecture to allow for configurability of each step and automate parameter selection. This project had very little ethical concerns in my consideration given that the binary classifier is making a suggestion to modify sentences based on level of difficulty. That seems free of ethical concern. Every ML model has implications on human beings so depending on how this model is used it could have an ethical dilemma. For instance if a publishing house is using the model to analyze unpublished books","title":"Part2"},{"location":"part2/#partb-unsupervised-learning","text":"","title":"PartB Unsupervised Learning"},{"location":"part2/#motivation","text":"In this part of the project our goal was to use Latent Dirichlet Allocation to \"cluster\" or create topcis which aggregate the documents in the dataset together for analysis. Motivation for the work in this section was to understand how to be use a topic model to extract a useful feature to then be able to fuel the classifier in Part A of this project. In doing so more information can be gleaned from understanding word pairings and probabilities distributions of documents to derived topics. More analysis is needed to use the topic as a feature for the binary classifier in Part A. This is discussed in the Stretch Goals on how this could have been accomplished.","title":"Motivation"},{"location":"part2/#data-source","text":"The sources for this data were provided by the instruction team for SIADS694 and SIADS695. Details on the dataset can be found in the kaggle competition . The dataset was downloaded using the kaggle dowloader python tool from the kaggle competition website. The dataset consisted of a few different files that were designed to be used as potential features for PartA. For PartB however we only needed to use the csv files with the sentences in them. Those files are named 'WikiLarge_Test.csv' and 'WikiLarge_Train.csv'. In the 'WikiLarge_Train.csv' there are two columns one called 'original_text' and one called 'label'. Original text is a string representing the document and label is its classification as an integer (0,1) as either 'does NOT need to be simplified' or 'does need to be simplified'. * 'WikiLarge_Train.csv' has 416,768 documents in the corpus (labeled) * 'WikiLarge_Test.csv' has 119,092 documents in the corpus (unlabeled)","title":"Data Source"},{"location":"part2/#unsupervised-learning-methods","text":"Our main goal here was to process the training documents to extract feature representations for PartA. Given time constraints this goal was not achieved fully. However it did yield some interesting findings going through the effort. It was a great challenge engineering wise working with a large dataset and performing some of the analysis needed to extract topics using LDA. Model tuning and other processing steps came at somewhat of a premium as well given how long it takes to make some of the steps.","title":"Unsupervised Learning Methods"},{"location":"part2/#source-code-workflow","text":"The methodology was similar to most Natural Language Processing tasks. There is a fairly discrete high level pipeline with potential modifications at each step that are dependent on the task at hand. Starting with the high level architecture. Given the amount of time spent coding and recoding or pickling objects a set of utilities were built in the form of a python package or library. It is title m2lib and is in the main project directory. It houses the picklers, preprocessors, featureizers, and model creating code.","title":"Source Code Workflow"},{"location":"part2/#pickling","text":"An important note here is that at each step instead of re-performing each sub step in the process great effort was taken to ensure that repeating the process was not necessary. Custom classes were built such that when inherited from they could check for presence of a pickle corresponding to the child class before initializing the object. This meant the whole class (object) would get serialized and deserialized automatically on init. Which isn't always desireable given that those classes may change when you are developing. So some work is needed to refine the architecture. A shortcut was taken to create a separate picklable object that could store the important output of each step. In the cases encountered here it was typically 5 types. Preprocessed tokens example # tokens ['the', 'car', 'was', 'fast'] #ngamrms (bigrams) ['the_car', 'car_was', 'was_fast'] # full token token + bigram ['the', 'car', 'was', 'fast', 'the_car', 'car_was', 'was_fast'] Featureized Tokens # bag of words (id, count) [(13, 1), (53, 3), (18, 4), (52, 3)] #TFID vectors [[[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]] ['addition', 'anna', 'austen', 'continued', 'continued_work'] # dictionary (gensim utils) Models (objects) HTML (charts) Native Python objects","title":"Pickling"},{"location":"part2/#basic-pipeline-steps","text":"Read data Preprocess (tokenize, lemmatize, stopwords, punctuation removal, etc) Create feature representations (Bag of words, vectors) Create Model Analyze Model Visualize","title":"Basic Pipeline Steps"},{"location":"part2/#overview","text":"Step 1 we are reading the data into memory using pandas read from csv functionality. Its not a particular costly process and can be done quickly. Still a class was created to handle reading and accessing dataframes and original data then serialized to pickle. Step 2 is preprocessing it was done mostly with gensim preprocessing classes that were very useful and comprehensible. SpaCy was a bit more difficult to fit into the design without going to far into their architecture. However given that is has some very advanced techniques for processing data it will be considered for future use. Their pre built pipelines are supposed to be fast and intelligent. Tokenizing code. As you can see two popular steps for tokenizing were left out which is stripping short tokens and stemming text. This decision was made for cleanliness of output from LDA model having legible words. def tokenize_gensim_string(self, doc): CUSTOM_FILTERS = [ strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, # strip_short, # stem_text ] doc_ = preprocess_string(doc, CUSTOM_FILTERS) return doc_ Step 3 Creating feature representation was also done with gensim using the bag of words model and TFID model. Each of which produce their own object representations as output that can be used for LDA model creation. See gensim tfidmodel and doc2bow Step 4 Creating models again was another task covered by gensim. LDA Model It's one of the more widely used for LDA and has tons of support. A model was created with bag of words and tfid to compare the differences in the two. A third model was created using Gibbs Sampling Dirichlet Multinomial Mixture which is supposed to be better at topic modeling for short texts. It is also known as the Movie Group Process. Described in this medium post . Step 5 Analyzing the models was done with using two metrics perplexity and coherence measures using gensim's LDA model attributes and methods. Step 6 Visualizations were done primarily with PyLDAvis . It can hook right into the model and provide insight into the 2D cluster structure of the topic model.","title":"Overview"},{"location":"part2/#parameter-tuning","text":"The next section discusses a little in more detail on evaluation and how it impacted parameter tuning. There were few parameters to tune that had meaningful effects on the outcomes. The most impactful parameter is by far the number of K topics selected for each topic model (LDA, GSDMM). For bag of words feature there are some parameters to filter out extreme values from the dataset that are either lower or higher in instance count. LDA model has a few parameters that can be tuned most were left default. self.lda_args = { 'chunksize': 2000, 'alpha' : 'auto', 'eta': 'auto', 'iterations' : 50, 'num_topics' : 10, # K 'passes' : 20, 'eval_every' : None, # 'workers' : 3, } To include or not include ngrams? That question was difficult to answer with higher order ngrams your doc term matrix ends up being very sparse. So I split the middle and went with only 2 ngram model. Once this was selected it remained stationary for the project as running the bag of words model was costly.","title":"Parameter Tuning"},{"location":"part2/#unsupervised-evaluation-and-parameter-tuning","text":"Evaluation was done at a few levels in the basic pipeline described above for picking some of the parameters and hyperparameters. However the largest amount of effort went into evaluating perplexity and coherence. Higher coherence is better and lower perplexity is better. Given time constraints only a few runs were made at trying to select an ideal K number of topics. The algorithm performed was to iterate through 4 topic sizes [5,10,15,20] to then calculate perplexity and coherence and optimize the topic selection based on the outcomes.","title":"Unsupervised Evaluation and Parameter Tuning"},{"location":"part2/#discussion","text":"There were several learnings from this project. One of the greatest is that large datasets are very difficult and time consuming to running analysis on. The gensim implementation of bag of words calculation took 10+ hours to complete on the final run. So pickling was a high priority in order to prevent from having to re run the pipeline steps for testing. The solution has several opportunities for expansion one mentioned in the section for stretch goals . Second the project overall could be integrated partA and partB in order to build a toolset that can run a full semisupervised pipeline with feature enhancements using topics. Lastly expand the pipeline, class structure, and architecture to allow for configurability of each step and automate parameter selection. This project had very little ethical concerns in my consideration given that the binary classifier is making a suggestion to modify sentences based on level of difficulty. That seems free of ethical concern. Every ML model has implications on human beings so depending on how this model is used it could have an ethical dilemma. For instance if a publishing house is using the model to analyze unpublished books","title":"Discussion"},{"location":"project/","text":"","title":"Project"},{"location":"stretchgoals/","text":"Stretch Goals Semisupervised Learning using LDA Topic Modeling Unfortunately this portion of the assignment went incomplete. However in theory this is what could have been accomplished.","title":"Stretch Goals"},{"location":"stretchgoals/#stretch-goals","text":"","title":"Stretch Goals"},{"location":"stretchgoals/#semisupervised-learning-using-lda-topic-modeling","text":"Unfortunately this portion of the assignment went incomplete. However in theory this is what could have been accomplished.","title":"Semisupervised Learning using LDA Topic Modeling"}]}